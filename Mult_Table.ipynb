{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMpys1SwCBBNStIVEa829F4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanAI7060/Something-new/blob/main/Mult_Table.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Generate training data\n",
        "def generate_data():\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(1, 13):\n",
        "        for j in range(1, 13):\n",
        "            inputs.append([i, j])\n",
        "            targets.append([i * j])\n",
        "    return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network model with three hidden layers\n",
        "class MultiplierNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiplierNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)  # Input layer with 2 neurons, 1st hidden layer with 16 neurons\n",
        "        self.fc2 = nn.Linear(16, 16) # 2nd hidden layer with 16 neurons\n",
        "        self.fc3 = nn.Linear(16, 16) # 3rd hidden layer with 16 neurons\n",
        "        self.fc4 = nn.Linear(16, 1)  # Output layer with 1 neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Training the model with early stopping based on loss threshold\n",
        "def train(model, criterion, optimizer, inputs, targets, epochs, loss_threshold):\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if loss.item() < loss_threshold:\n",
        "            print(f'Epoch [{epoch+1}], Early stopping with loss: {loss.item():.4f}')\n",
        "            break\n",
        "        elif (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    inputs, targets = generate_data()\n",
        "    model = MultiplierNN()\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    epochs = int(input(\"Enter the number of epochs for training: \")) # try 200,000 epecs and stop loss of 0.0204\n",
        "    loss_threshold = float(input(\"Enter the loss threshold for early stopping: \"))\n",
        "    train(model, criterion, optimizer, inputs, targets, epochs, loss_threshold)\n",
        "\n",
        "    model.eval()\n",
        "    while True:\n",
        "        num1 = int(input(\"Enter the first number (1-12, or 0 to stop): \"))\n",
        "        num2 = int(input(\"Enter the second number (1-12, or 0 to stop): \"))\n",
        "        if num1 == 0 and num2 == 0:\n",
        "            print(\"Exiting testing loop.\")\n",
        "            break\n",
        "        test_input = torch.tensor([[num1, num2]], dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            predicted = model(test_input)\n",
        "            print(f'The result of {num1} multiplied by {num2} is approximately: {predicted.item()}')\n",
        "            print(' ')\n",
        "            rounded_predicted_value = round(predicted.item())\n",
        "            print(f'The result of {num1} multiplied by {num2} is approximately: {rounded_predicted_value}')\n",
        "            print('***************************************')\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Generate training data\n",
        "def generate_data():\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in range(1, 13):\n",
        "        for j in range(1, 13):\n",
        "            inputs.append([i, j])\n",
        "            targets.append([i * j])\n",
        "    return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "# Define the neural network model with three hidden layers\n",
        "class MultiplierNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiplierNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)  # Input layer with 2 neurons, 1st hidden layer with 16 neurons\n",
        "        self.fc2 = nn.Linear(16, 16) # 2nd hidden layer with 16 neurons\n",
        "        self.fc3 = nn.Linear(16, 16) # 3rd hidden layer with 16 neurons\n",
        "        self.fc4 = nn.Linear(16, 1)  # Output layer with 1 neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Training the model with early stopping based on loss threshold\n",
        "def train(model, criterion, optimizer, inputs, targets, epochs, loss_threshold):\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if loss.item() < loss_threshold:\n",
        "            print(f'Epoch [{epoch+1}], Early stopping with loss: {loss.item():.4f}')\n",
        "            break\n",
        "        elif (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    inputs, targets = generate_data()\n",
        "    model = MultiplierNN()\n",
        "    criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    epochs = int(input(\"Enter the number of epochs for training: \")) # try 200,000 epecs and stop loss of 0.0204\n",
        "    loss_threshold = float(input(\"Enter the loss threshold for early stopping: \"))\n",
        "    train(model, criterion, optimizer, inputs, targets, epochs, loss_threshold)\n",
        "\n",
        "    model.eval()\n",
        "    while True:\n",
        "        num1 = int(input(\"Enter the first number (1-12, or 0 to stop): \"))\n",
        "        num2 = int(input(\"Enter the second number (1-12, or 0 to stop): \"))\n",
        "        if num1 == 0 and num2 == 0:\n",
        "            print(\"Exiting testing loop.\")\n",
        "            break\n",
        "        test_input = torch.tensor([[num1, num2]], dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            predicted = model(test_input)\n",
        "            print(f'The result of {num1} multiplied by {num2} is approximately: {predicted.item()}')\n",
        "            print(' ')\n",
        "            rounded_predicted_value = round(predicted.item())\n",
        "            print(f'The result of {num1} multiplied by {num2} is approximately: {rounded_predicted_value}')\n",
        "            print('***************************************')\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1vcaUlXktRi",
        "outputId": "a2c6866b-6398-4efe-bfae-4611f2ec4cbc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of epochs for training: 200000\n",
            "Enter the loss threshold for early stopping: 0.0204\n",
            "Epoch [100/200000], Loss: 92.9989\n",
            "Epoch [200/200000], Loss: 11.5004\n",
            "Epoch [300/200000], Loss: 3.3746\n",
            "Epoch [400/200000], Loss: 2.0025\n",
            "Epoch [500/200000], Loss: 1.5746\n",
            "Epoch [600/200000], Loss: 1.0670\n",
            "Epoch [700/200000], Loss: 0.8333\n",
            "Epoch [800/200000], Loss: 0.6944\n",
            "Epoch [900/200000], Loss: 0.5988\n",
            "Epoch [1000/200000], Loss: 0.5236\n",
            "Epoch [1100/200000], Loss: 0.5159\n",
            "Epoch [1200/200000], Loss: 0.5208\n",
            "Epoch [1300/200000], Loss: 0.4826\n",
            "Epoch [1400/200000], Loss: 0.5207\n",
            "Epoch [1500/200000], Loss: 0.3085\n",
            "Epoch [1600/200000], Loss: 0.6031\n",
            "Epoch [1700/200000], Loss: 0.2700\n",
            "Epoch [1800/200000], Loss: 0.3045\n",
            "Epoch [1900/200000], Loss: 0.5502\n",
            "Epoch [2000/200000], Loss: 0.3030\n",
            "Epoch [2100/200000], Loss: 0.2200\n",
            "Epoch [2200/200000], Loss: 0.3450\n",
            "Epoch [2300/200000], Loss: 1.2645\n",
            "Epoch [2400/200000], Loss: 0.3146\n",
            "Epoch [2500/200000], Loss: 0.4628\n",
            "Epoch [2600/200000], Loss: 0.1693\n",
            "Epoch [2700/200000], Loss: 0.1815\n",
            "Epoch [2800/200000], Loss: 0.4345\n",
            "Epoch [2900/200000], Loss: 0.1768\n",
            "Epoch [3000/200000], Loss: 0.5492\n",
            "Epoch [3100/200000], Loss: 0.3928\n",
            "Epoch [3200/200000], Loss: 0.3161\n",
            "Epoch [3300/200000], Loss: 0.4644\n",
            "Epoch [3400/200000], Loss: 0.2955\n",
            "Epoch [3500/200000], Loss: 0.1832\n",
            "Epoch [3600/200000], Loss: 0.4005\n",
            "Epoch [3700/200000], Loss: 0.2408\n",
            "Epoch [3800/200000], Loss: 0.2684\n",
            "Epoch [3900/200000], Loss: 0.7542\n",
            "Epoch [4000/200000], Loss: 0.2119\n",
            "Epoch [4100/200000], Loss: 0.2094\n",
            "Epoch [4200/200000], Loss: 0.2470\n",
            "Epoch [4300/200000], Loss: 0.2040\n",
            "Epoch [4400/200000], Loss: 0.1232\n",
            "Epoch [4500/200000], Loss: 0.1426\n",
            "Epoch [4600/200000], Loss: 0.1143\n",
            "Epoch [4700/200000], Loss: 0.2410\n",
            "Epoch [4800/200000], Loss: 0.1134\n",
            "Epoch [4900/200000], Loss: 0.6617\n",
            "Epoch [5000/200000], Loss: 0.3786\n",
            "Epoch [5100/200000], Loss: 0.1563\n",
            "Epoch [5200/200000], Loss: 0.1656\n",
            "Epoch [5300/200000], Loss: 0.1456\n",
            "Epoch [5400/200000], Loss: 0.4577\n",
            "Epoch [5500/200000], Loss: 0.1125\n",
            "Epoch [5600/200000], Loss: 0.1678\n",
            "Epoch [5700/200000], Loss: 0.1001\n",
            "Epoch [5800/200000], Loss: 0.2024\n",
            "Epoch [5900/200000], Loss: 0.3345\n",
            "Epoch [6000/200000], Loss: 0.1053\n",
            "Epoch [6100/200000], Loss: 0.3607\n",
            "Epoch [6200/200000], Loss: 0.3095\n",
            "Epoch [6300/200000], Loss: 1.1161\n",
            "Epoch [6400/200000], Loss: 0.1369\n",
            "Epoch [6500/200000], Loss: 0.1151\n",
            "Epoch [6600/200000], Loss: 0.1341\n",
            "Epoch [6700/200000], Loss: 0.1132\n",
            "Epoch [6800/200000], Loss: 0.1110\n",
            "Epoch [6900/200000], Loss: 0.0900\n",
            "Epoch [7000/200000], Loss: 0.3547\n",
            "Epoch [7100/200000], Loss: 0.0855\n",
            "Epoch [7200/200000], Loss: 0.0828\n",
            "Epoch [7300/200000], Loss: 0.0956\n",
            "Epoch [7400/200000], Loss: 0.2852\n",
            "Epoch [7500/200000], Loss: 0.1569\n",
            "Epoch [7600/200000], Loss: 0.2043\n",
            "Epoch [7700/200000], Loss: 0.1337\n",
            "Epoch [7800/200000], Loss: 1.3264\n",
            "Epoch [7900/200000], Loss: 0.0800\n",
            "Epoch [8000/200000], Loss: 0.2503\n",
            "Epoch [8100/200000], Loss: 0.4617\n",
            "Epoch [8200/200000], Loss: 0.2865\n",
            "Epoch [8300/200000], Loss: 0.3360\n",
            "Epoch [8400/200000], Loss: 0.2345\n",
            "Epoch [8500/200000], Loss: 0.2898\n",
            "Epoch [8600/200000], Loss: 0.3542\n",
            "Epoch [8700/200000], Loss: 0.4810\n",
            "Epoch [8800/200000], Loss: 0.0799\n",
            "Epoch [8900/200000], Loss: 0.0873\n",
            "Epoch [9000/200000], Loss: 0.2683\n",
            "Epoch [9100/200000], Loss: 0.1344\n",
            "Epoch [9200/200000], Loss: 0.5711\n",
            "Epoch [9300/200000], Loss: 0.0825\n",
            "Epoch [9400/200000], Loss: 0.0903\n",
            "Epoch [9500/200000], Loss: 0.1297\n",
            "Epoch [9600/200000], Loss: 0.1162\n",
            "Epoch [9700/200000], Loss: 0.0798\n",
            "Epoch [9800/200000], Loss: 0.4165\n",
            "Epoch [9900/200000], Loss: 0.1181\n",
            "Epoch [10000/200000], Loss: 0.2910\n",
            "Epoch [10100/200000], Loss: 0.2236\n",
            "Epoch [10200/200000], Loss: 0.7930\n",
            "Epoch [10300/200000], Loss: 0.0795\n",
            "Epoch [10400/200000], Loss: 0.0884\n",
            "Epoch [10500/200000], Loss: 0.1803\n",
            "Epoch [10600/200000], Loss: 0.4414\n",
            "Epoch [10700/200000], Loss: 0.1429\n",
            "Epoch [10800/200000], Loss: 0.0821\n",
            "Epoch [10900/200000], Loss: 0.0778\n",
            "Epoch [11000/200000], Loss: 0.1726\n",
            "Epoch [11100/200000], Loss: 0.0841\n",
            "Epoch [11200/200000], Loss: 0.3035\n",
            "Epoch [11300/200000], Loss: 0.1395\n",
            "Epoch [11400/200000], Loss: 0.1179\n",
            "Epoch [11500/200000], Loss: 0.0799\n",
            "Epoch [11600/200000], Loss: 0.0779\n",
            "Epoch [11700/200000], Loss: 0.0809\n",
            "Epoch [11800/200000], Loss: 0.1476\n",
            "Epoch [11900/200000], Loss: 0.5616\n",
            "Epoch [12000/200000], Loss: 0.1645\n",
            "Epoch [12100/200000], Loss: 0.1629\n",
            "Epoch [12200/200000], Loss: 0.1578\n",
            "Epoch [12300/200000], Loss: 0.1690\n",
            "Epoch [12400/200000], Loss: 0.0896\n",
            "Epoch [12500/200000], Loss: 0.5356\n",
            "Epoch [12600/200000], Loss: 0.2247\n",
            "Epoch [12700/200000], Loss: 0.2584\n",
            "Epoch [12800/200000], Loss: 0.1124\n",
            "Epoch [12900/200000], Loss: 0.0785\n",
            "Epoch [13000/200000], Loss: 0.0909\n",
            "Epoch [13100/200000], Loss: 0.3711\n",
            "Epoch [13200/200000], Loss: 0.1372\n",
            "Epoch [13300/200000], Loss: 0.1477\n",
            "Epoch [13400/200000], Loss: 0.0749\n",
            "Epoch [13500/200000], Loss: 0.1086\n",
            "Epoch [13600/200000], Loss: 0.2222\n",
            "Epoch [13700/200000], Loss: 0.0971\n",
            "Epoch [13800/200000], Loss: 0.3406\n",
            "Epoch [13900/200000], Loss: 0.0916\n",
            "Epoch [14000/200000], Loss: 0.0772\n",
            "Epoch [14100/200000], Loss: 0.0851\n",
            "Epoch [14200/200000], Loss: 0.8047\n",
            "Epoch [14300/200000], Loss: 0.1328\n",
            "Epoch [14400/200000], Loss: 0.3708\n",
            "Epoch [14500/200000], Loss: 0.1143\n",
            "Epoch [14600/200000], Loss: 0.3223\n",
            "Epoch [14700/200000], Loss: 0.3136\n",
            "Epoch [14800/200000], Loss: 0.1975\n",
            "Epoch [14900/200000], Loss: 0.3345\n",
            "Epoch [15000/200000], Loss: 0.0900\n",
            "Epoch [15100/200000], Loss: 0.2873\n",
            "Epoch [15200/200000], Loss: 1.0420\n",
            "Epoch [15300/200000], Loss: 0.0739\n",
            "Epoch [15400/200000], Loss: 0.0740\n",
            "Epoch [15500/200000], Loss: 0.0877\n",
            "Epoch [15600/200000], Loss: 0.3806\n",
            "Epoch [15700/200000], Loss: 0.1734\n",
            "Epoch [15800/200000], Loss: 0.1000\n",
            "Epoch [15900/200000], Loss: 0.1552\n",
            "Epoch [16000/200000], Loss: 0.0754\n",
            "Epoch [16100/200000], Loss: 0.0713\n",
            "Epoch [16200/200000], Loss: 0.0711\n",
            "Epoch [16300/200000], Loss: 0.0754\n",
            "Epoch [16400/200000], Loss: 0.1866\n",
            "Epoch [16500/200000], Loss: 0.0810\n",
            "Epoch [16600/200000], Loss: 0.1082\n",
            "Epoch [16700/200000], Loss: 0.0761\n",
            "Epoch [16800/200000], Loss: 1.3163\n",
            "Epoch [16900/200000], Loss: 0.2017\n",
            "Epoch [17000/200000], Loss: 0.0806\n",
            "Epoch [17100/200000], Loss: 0.2023\n",
            "Epoch [17200/200000], Loss: 0.0722\n",
            "Epoch [17300/200000], Loss: 0.0710\n",
            "Epoch [17400/200000], Loss: 0.0715\n",
            "Epoch [17500/200000], Loss: 0.7948\n",
            "Epoch [17600/200000], Loss: 0.0751\n",
            "Epoch [17700/200000], Loss: 0.0887\n",
            "Epoch [17800/200000], Loss: 0.1214\n",
            "Epoch [17900/200000], Loss: 0.0725\n",
            "Epoch [18000/200000], Loss: 0.1494\n",
            "Epoch [18100/200000], Loss: 0.0867\n",
            "Epoch [18200/200000], Loss: 0.0712\n",
            "Epoch [18300/200000], Loss: 0.3406\n",
            "Epoch [18400/200000], Loss: 0.1273\n",
            "Epoch [18500/200000], Loss: 0.1132\n",
            "Epoch [18600/200000], Loss: 0.1156\n",
            "Epoch [18700/200000], Loss: 0.0749\n",
            "Epoch [18800/200000], Loss: 0.2006\n",
            "Epoch [18900/200000], Loss: 0.1291\n",
            "Epoch [19000/200000], Loss: 0.0728\n",
            "Epoch [19100/200000], Loss: 0.2241\n",
            "Epoch [19200/200000], Loss: 0.5241\n",
            "Epoch [19300/200000], Loss: 0.0842\n",
            "Epoch [19400/200000], Loss: 1.1788\n",
            "Epoch [19500/200000], Loss: 0.2064\n",
            "Epoch [19600/200000], Loss: 0.0755\n",
            "Epoch [19700/200000], Loss: 0.0964\n",
            "Epoch [19800/200000], Loss: 0.0747\n",
            "Epoch [19900/200000], Loss: 0.0854\n",
            "Epoch [20000/200000], Loss: 1.4290\n",
            "Epoch [20100/200000], Loss: 0.1475\n",
            "Epoch [20200/200000], Loss: 0.1060\n",
            "Epoch [20300/200000], Loss: 0.3571\n",
            "Epoch [20400/200000], Loss: 0.0863\n",
            "Epoch [20500/200000], Loss: 0.0718\n",
            "Epoch [20600/200000], Loss: 0.0697\n",
            "Epoch [20700/200000], Loss: 0.0726\n",
            "Epoch [20800/200000], Loss: 0.1025\n",
            "Epoch [20900/200000], Loss: 0.0711\n",
            "Epoch [21000/200000], Loss: 0.0699\n",
            "Epoch [21100/200000], Loss: 0.6986\n",
            "Epoch [21200/200000], Loss: 0.0711\n",
            "Epoch [21300/200000], Loss: 0.0699\n",
            "Epoch [21400/200000], Loss: 0.4523\n",
            "Epoch [21500/200000], Loss: 0.0690\n",
            "Epoch [21600/200000], Loss: 0.5726\n",
            "Epoch [21700/200000], Loss: 0.3305\n",
            "Epoch [21800/200000], Loss: 0.0727\n",
            "Epoch [21900/200000], Loss: 0.2149\n",
            "Epoch [22000/200000], Loss: 0.0822\n",
            "Epoch [22100/200000], Loss: 0.0736\n",
            "Epoch [22200/200000], Loss: 0.2310\n",
            "Epoch [22300/200000], Loss: 0.0900\n",
            "Epoch [22400/200000], Loss: 0.0717\n",
            "Epoch [22500/200000], Loss: 0.0722\n",
            "Epoch [22600/200000], Loss: 0.1013\n",
            "Epoch [22700/200000], Loss: 0.1037\n",
            "Epoch [22800/200000], Loss: 0.4253\n",
            "Epoch [22900/200000], Loss: 0.2011\n",
            "Epoch [23000/200000], Loss: 0.1131\n",
            "Epoch [23100/200000], Loss: 0.0996\n",
            "Epoch [23200/200000], Loss: 0.0727\n",
            "Epoch [23300/200000], Loss: 0.0849\n",
            "Epoch [23400/200000], Loss: 0.1830\n",
            "Epoch [23500/200000], Loss: 0.1596\n",
            "Epoch [23600/200000], Loss: 0.1294\n",
            "Epoch [23700/200000], Loss: 0.0862\n",
            "Epoch [23800/200000], Loss: 0.0957\n",
            "Epoch [23900/200000], Loss: 0.2639\n",
            "Epoch [24000/200000], Loss: 0.0884\n",
            "Epoch [24100/200000], Loss: 0.1522\n",
            "Epoch [24200/200000], Loss: 0.1050\n",
            "Epoch [24300/200000], Loss: 0.0821\n",
            "Epoch [24400/200000], Loss: 0.2277\n",
            "Epoch [24500/200000], Loss: 0.0791\n",
            "Epoch [24600/200000], Loss: 0.4032\n",
            "Epoch [24700/200000], Loss: 0.0718\n",
            "Epoch [24800/200000], Loss: 0.9892\n",
            "Epoch [24900/200000], Loss: 0.0927\n",
            "Epoch [25000/200000], Loss: 0.2471\n",
            "Epoch [25100/200000], Loss: 0.0798\n",
            "Epoch [25200/200000], Loss: 0.1313\n",
            "Epoch [25300/200000], Loss: 0.0697\n",
            "Epoch [25400/200000], Loss: 0.1628\n",
            "Epoch [25500/200000], Loss: 0.0684\n",
            "Epoch [25600/200000], Loss: 0.0686\n",
            "Epoch [25700/200000], Loss: 0.4336\n",
            "Epoch [25800/200000], Loss: 0.0716\n",
            "Epoch [25900/200000], Loss: 0.1699\n",
            "Epoch [26000/200000], Loss: 0.0723\n",
            "Epoch [26100/200000], Loss: 0.1101\n",
            "Epoch [26200/200000], Loss: 0.0695\n",
            "Epoch [26300/200000], Loss: 0.0780\n",
            "Epoch [26400/200000], Loss: 0.0726\n",
            "Epoch [26500/200000], Loss: 0.2828\n",
            "Epoch [26600/200000], Loss: 0.2515\n",
            "Epoch [26700/200000], Loss: 0.1087\n",
            "Epoch [26800/200000], Loss: 0.0723\n",
            "Epoch [26900/200000], Loss: 0.3324\n",
            "Epoch [27000/200000], Loss: 0.1052\n",
            "Epoch [27100/200000], Loss: 0.1615\n",
            "Epoch [27200/200000], Loss: 0.0972\n",
            "Epoch [27300/200000], Loss: 0.0695\n",
            "Epoch [27400/200000], Loss: 0.0672\n",
            "Epoch [27500/200000], Loss: 0.0681\n",
            "Epoch [27600/200000], Loss: 0.4648\n",
            "Epoch [27700/200000], Loss: 0.1048\n",
            "Epoch [27800/200000], Loss: 0.0942\n",
            "Epoch [27900/200000], Loss: 0.0700\n",
            "Epoch [28000/200000], Loss: 0.1241\n",
            "Epoch [28100/200000], Loss: 0.0703\n",
            "Epoch [28200/200000], Loss: 0.6979\n",
            "Epoch [28300/200000], Loss: 0.0695\n",
            "Epoch [28400/200000], Loss: 0.0672\n",
            "Epoch [28500/200000], Loss: 0.1179\n",
            "Epoch [28600/200000], Loss: 0.0687\n",
            "Epoch [28700/200000], Loss: 0.4202\n",
            "Epoch [28800/200000], Loss: 0.0725\n",
            "Epoch [28900/200000], Loss: 0.0676\n",
            "Epoch [29000/200000], Loss: 0.0808\n",
            "Epoch [29100/200000], Loss: 0.5040\n",
            "Epoch [29200/200000], Loss: 0.1117\n",
            "Epoch [29300/200000], Loss: 0.0801\n",
            "Epoch [29400/200000], Loss: 0.0848\n",
            "Epoch [29500/200000], Loss: 0.0739\n",
            "Epoch [29600/200000], Loss: 0.0695\n",
            "Epoch [29700/200000], Loss: 0.1009\n",
            "Epoch [29800/200000], Loss: 0.2429\n",
            "Epoch [29900/200000], Loss: 0.2007\n",
            "Epoch [30000/200000], Loss: 0.1133\n",
            "Epoch [30100/200000], Loss: 0.0867\n",
            "Epoch [30200/200000], Loss: 0.2233\n",
            "Epoch [30300/200000], Loss: 0.0713\n",
            "Epoch [30400/200000], Loss: 0.0691\n",
            "Epoch [30500/200000], Loss: 0.2766\n",
            "Epoch [30600/200000], Loss: 0.1535\n",
            "Epoch [30700/200000], Loss: 0.0670\n",
            "Epoch [30800/200000], Loss: 0.0693\n",
            "Epoch [30900/200000], Loss: 0.0676\n",
            "Epoch [31000/200000], Loss: 0.2393\n",
            "Epoch [31100/200000], Loss: 0.0907\n",
            "Epoch [31200/200000], Loss: 0.1046\n",
            "Epoch [31300/200000], Loss: 0.7222\n",
            "Epoch [31400/200000], Loss: 0.0716\n",
            "Epoch [31500/200000], Loss: 0.2717\n",
            "Epoch [31600/200000], Loss: 0.0668\n",
            "Epoch [31700/200000], Loss: 0.1400\n",
            "Epoch [31800/200000], Loss: 0.1438\n",
            "Epoch [31900/200000], Loss: 0.0703\n",
            "Epoch [32000/200000], Loss: 0.4730\n",
            "Epoch [32100/200000], Loss: 0.0735\n",
            "Epoch [32200/200000], Loss: 0.0827\n",
            "Epoch [32300/200000], Loss: 0.0859\n",
            "Epoch [32400/200000], Loss: 0.0885\n",
            "Epoch [32500/200000], Loss: 0.7529\n",
            "Epoch [32600/200000], Loss: 0.2026\n",
            "Epoch [32700/200000], Loss: 0.2974\n",
            "Epoch [32800/200000], Loss: 0.0808\n",
            "Epoch [32900/200000], Loss: 0.0846\n",
            "Epoch [33000/200000], Loss: 0.6695\n",
            "Epoch [33100/200000], Loss: 0.0848\n",
            "Epoch [33200/200000], Loss: 0.0905\n",
            "Epoch [33300/200000], Loss: 0.0743\n",
            "Epoch [33400/200000], Loss: 0.3841\n",
            "Epoch [33500/200000], Loss: 0.3076\n",
            "Epoch [33600/200000], Loss: 0.0920\n",
            "Epoch [33700/200000], Loss: 0.0805\n",
            "Epoch [33800/200000], Loss: 0.9641\n",
            "Epoch [33900/200000], Loss: 0.0665\n",
            "Epoch [34000/200000], Loss: 0.2153\n",
            "Epoch [34100/200000], Loss: 0.0668\n",
            "Epoch [34200/200000], Loss: 0.0807\n",
            "Epoch [34300/200000], Loss: 0.2874\n",
            "Epoch [34400/200000], Loss: 0.0909\n",
            "Epoch [34500/200000], Loss: 0.1873\n",
            "Epoch [34600/200000], Loss: 0.0695\n",
            "Epoch [34700/200000], Loss: 0.2079\n",
            "Epoch [34800/200000], Loss: 0.0748\n",
            "Epoch [34900/200000], Loss: 0.0695\n",
            "Epoch [35000/200000], Loss: 0.0746\n",
            "Epoch [35100/200000], Loss: 0.1789\n",
            "Epoch [35200/200000], Loss: 0.0693\n",
            "Epoch [35300/200000], Loss: 0.2864\n",
            "Epoch [35400/200000], Loss: 0.0666\n",
            "Epoch [35500/200000], Loss: 0.0692\n",
            "Epoch [35600/200000], Loss: 0.2143\n",
            "Epoch [35700/200000], Loss: 0.0731\n",
            "Epoch [35800/200000], Loss: 0.1624\n",
            "Epoch [35900/200000], Loss: 0.0852\n",
            "Epoch [36000/200000], Loss: 1.3428\n",
            "Epoch [36100/200000], Loss: 0.0816\n",
            "Epoch [36200/200000], Loss: 0.6517\n",
            "Epoch [36300/200000], Loss: 0.0670\n",
            "Epoch [36400/200000], Loss: 0.0678\n",
            "Epoch [36500/200000], Loss: 0.0744\n",
            "Epoch [36600/200000], Loss: 0.0671\n",
            "Epoch [36700/200000], Loss: 0.4014\n",
            "Epoch [36800/200000], Loss: 0.0662\n",
            "Epoch [36900/200000], Loss: 0.0881\n",
            "Epoch [37000/200000], Loss: 0.0697\n",
            "Epoch [37100/200000], Loss: 0.0820\n",
            "Epoch [37200/200000], Loss: 0.0723\n",
            "Epoch [37300/200000], Loss: 0.0686\n",
            "Epoch [37400/200000], Loss: 0.0665\n",
            "Epoch [37500/200000], Loss: 0.2597\n",
            "Epoch [37600/200000], Loss: 0.0850\n",
            "Epoch [37700/200000], Loss: 0.0752\n",
            "Epoch [37800/200000], Loss: 0.0754\n",
            "Epoch [37900/200000], Loss: 0.0690\n",
            "Epoch [38000/200000], Loss: 0.0725\n",
            "Epoch [38100/200000], Loss: 0.0812\n",
            "Epoch [38200/200000], Loss: 0.0822\n",
            "Epoch [38300/200000], Loss: 0.2560\n",
            "Epoch [38400/200000], Loss: 0.0953\n",
            "Epoch [38500/200000], Loss: 0.0833\n",
            "Epoch [38600/200000], Loss: 0.0816\n",
            "Epoch [38700/200000], Loss: 0.1434\n",
            "Epoch [38800/200000], Loss: 0.0816\n",
            "Epoch [38900/200000], Loss: 0.0660\n",
            "Epoch [39000/200000], Loss: 0.0659\n",
            "Epoch [39100/200000], Loss: 0.0682\n",
            "Epoch [39200/200000], Loss: 0.0656\n",
            "Epoch [39300/200000], Loss: 0.0653\n",
            "Epoch [39400/200000], Loss: 0.2165\n",
            "Epoch [39500/200000], Loss: 0.0732\n",
            "Epoch [39600/200000], Loss: 0.0907\n",
            "Epoch [39700/200000], Loss: 0.1033\n",
            "Epoch [39800/200000], Loss: 0.2022\n",
            "Epoch [39900/200000], Loss: 0.1007\n",
            "Epoch [40000/200000], Loss: 0.0948\n",
            "Epoch [40100/200000], Loss: 0.0808\n",
            "Epoch [40200/200000], Loss: 0.0667\n",
            "Epoch [40300/200000], Loss: 0.1659\n",
            "Epoch [40400/200000], Loss: 0.0650\n",
            "Epoch [40500/200000], Loss: 0.0964\n",
            "Epoch [40600/200000], Loss: 0.0861\n",
            "Epoch [40700/200000], Loss: 0.1106\n",
            "Epoch [40800/200000], Loss: 0.6618\n",
            "Epoch [40900/200000], Loss: 0.0749\n",
            "Epoch [41000/200000], Loss: 0.4258\n",
            "Epoch [41100/200000], Loss: 0.0744\n",
            "Epoch [41200/200000], Loss: 0.4723\n",
            "Epoch [41300/200000], Loss: 0.4447\n",
            "Epoch [41400/200000], Loss: 0.1020\n",
            "Epoch [41500/200000], Loss: 0.0666\n",
            "Epoch [41600/200000], Loss: 0.0810\n",
            "Epoch [41700/200000], Loss: 0.1468\n",
            "Epoch [41800/200000], Loss: 0.0728\n",
            "Epoch [41900/200000], Loss: 0.0700\n",
            "Epoch [42000/200000], Loss: 0.0684\n",
            "Epoch [42100/200000], Loss: 0.4298\n",
            "Epoch [42200/200000], Loss: 0.1083\n",
            "Epoch [42300/200000], Loss: 0.0683\n",
            "Epoch [42400/200000], Loss: 0.0660\n",
            "Epoch [42500/200000], Loss: 0.0686\n",
            "Epoch [42600/200000], Loss: 0.4669\n",
            "Epoch [42700/200000], Loss: 0.4584\n",
            "Epoch [42800/200000], Loss: 0.0757\n",
            "Epoch [42900/200000], Loss: 0.0792\n",
            "Epoch [43000/200000], Loss: 0.0674\n",
            "Epoch [43100/200000], Loss: 0.0888\n",
            "Epoch [43200/200000], Loss: 0.0636\n",
            "Epoch [43300/200000], Loss: 0.0730\n",
            "Epoch [43400/200000], Loss: 0.0592\n",
            "Epoch [43500/200000], Loss: 0.0583\n",
            "Epoch [43600/200000], Loss: 0.0900\n",
            "Epoch [43700/200000], Loss: 0.0679\n",
            "Epoch [43800/200000], Loss: 0.9944\n",
            "Epoch [43900/200000], Loss: 0.0691\n",
            "Epoch [44000/200000], Loss: 0.0789\n",
            "Epoch [44100/200000], Loss: 0.0914\n",
            "Epoch [44200/200000], Loss: 0.1807\n",
            "Epoch [44300/200000], Loss: 0.1639\n",
            "Epoch [44400/200000], Loss: 0.3042\n",
            "Epoch [44500/200000], Loss: 0.1441\n",
            "Epoch [44600/200000], Loss: 0.0498\n",
            "Epoch [44700/200000], Loss: 0.2680\n",
            "Epoch [44800/200000], Loss: 0.0552\n",
            "Epoch [44900/200000], Loss: 0.2018\n",
            "Epoch [45000/200000], Loss: 0.0445\n",
            "Epoch [45100/200000], Loss: 1.2064\n",
            "Epoch [45200/200000], Loss: 0.2202\n",
            "Epoch [45300/200000], Loss: 0.1657\n",
            "Epoch [45400/200000], Loss: 0.3197\n",
            "Epoch [45500/200000], Loss: 0.0560\n",
            "Epoch [45600/200000], Loss: 0.0502\n",
            "Epoch [45700/200000], Loss: 0.0948\n",
            "Epoch [45800/200000], Loss: 0.1514\n",
            "Epoch [45900/200000], Loss: 0.0781\n",
            "Epoch [46000/200000], Loss: 0.0438\n",
            "Epoch [46100/200000], Loss: 0.0788\n",
            "Epoch [46200/200000], Loss: 0.0636\n",
            "Epoch [46300/200000], Loss: 0.0832\n",
            "Epoch [46400/200000], Loss: 0.1394\n",
            "Epoch [46500/200000], Loss: 0.0550\n",
            "Epoch [46600/200000], Loss: 0.0573\n",
            "Epoch [46700/200000], Loss: 0.0453\n",
            "Epoch [46800/200000], Loss: 0.0607\n",
            "Epoch [46900/200000], Loss: 0.1275\n",
            "Epoch [47000/200000], Loss: 0.3558\n",
            "Epoch [47100/200000], Loss: 0.0554\n",
            "Epoch [47200/200000], Loss: 0.0760\n",
            "Epoch [47300/200000], Loss: 0.2046\n",
            "Epoch [47400/200000], Loss: 0.0700\n",
            "Epoch [47500/200000], Loss: 0.0394\n",
            "Epoch [47600/200000], Loss: 0.2910\n",
            "Epoch [47700/200000], Loss: 0.0508\n",
            "Epoch [47800/200000], Loss: 0.1722\n",
            "Epoch [47900/200000], Loss: 0.0709\n",
            "Epoch [48000/200000], Loss: 0.0518\n",
            "Epoch [48100/200000], Loss: 0.2153\n",
            "Epoch [48200/200000], Loss: 0.0391\n",
            "Epoch [48300/200000], Loss: 0.1441\n",
            "Epoch [48400/200000], Loss: 0.0921\n",
            "Epoch [48500/200000], Loss: 0.1592\n",
            "Epoch [48600/200000], Loss: 0.0614\n",
            "Epoch [48700/200000], Loss: 0.0442\n",
            "Epoch [48800/200000], Loss: 0.2257\n",
            "Epoch [48900/200000], Loss: 0.0698\n",
            "Epoch [49000/200000], Loss: 0.0662\n",
            "Epoch [49100/200000], Loss: 0.1748\n",
            "Epoch [49200/200000], Loss: 0.0507\n",
            "Epoch [49300/200000], Loss: 0.0510\n",
            "Epoch [49400/200000], Loss: 0.0390\n",
            "Epoch [49500/200000], Loss: 0.0706\n",
            "Epoch [49600/200000], Loss: 0.0409\n",
            "Epoch [49700/200000], Loss: 0.0507\n",
            "Epoch [49800/200000], Loss: 0.0409\n",
            "Epoch [49900/200000], Loss: 0.0412\n",
            "Epoch [50000/200000], Loss: 0.2288\n",
            "Epoch [50100/200000], Loss: 0.0348\n",
            "Epoch [50200/200000], Loss: 0.1368\n",
            "Epoch [50300/200000], Loss: 0.0510\n",
            "Epoch [50400/200000], Loss: 0.0594\n",
            "Epoch [50500/200000], Loss: 0.0951\n",
            "Epoch [50600/200000], Loss: 0.1541\n",
            "Epoch [50700/200000], Loss: 0.1076\n",
            "Epoch [50800/200000], Loss: 0.1642\n",
            "Epoch [50900/200000], Loss: 0.0628\n",
            "Epoch [51000/200000], Loss: 0.0901\n",
            "Epoch [51100/200000], Loss: 0.0334\n",
            "Epoch [51200/200000], Loss: 0.6377\n",
            "Epoch [51300/200000], Loss: 0.3668\n",
            "Epoch [51400/200000], Loss: 0.0794\n",
            "Epoch [51500/200000], Loss: 0.1213\n",
            "Epoch [51600/200000], Loss: 0.0402\n",
            "Epoch [51700/200000], Loss: 0.0404\n",
            "Epoch [51800/200000], Loss: 0.4196\n",
            "Epoch [51900/200000], Loss: 0.0578\n",
            "Epoch [52000/200000], Loss: 0.1408\n",
            "Epoch [52100/200000], Loss: 0.0368\n",
            "Epoch [52200/200000], Loss: 0.0665\n",
            "Epoch [52300/200000], Loss: 0.0537\n",
            "Epoch [52400/200000], Loss: 0.0708\n",
            "Epoch [52500/200000], Loss: 0.0550\n",
            "Epoch [52600/200000], Loss: 0.1066\n",
            "Epoch [52700/200000], Loss: 0.0384\n",
            "Epoch [52800/200000], Loss: 0.0422\n",
            "Epoch [52900/200000], Loss: 0.0399\n",
            "Epoch [53000/200000], Loss: 0.1226\n",
            "Epoch [53100/200000], Loss: 0.0366\n",
            "Epoch [53200/200000], Loss: 0.0628\n",
            "Epoch [53300/200000], Loss: 0.0598\n",
            "Epoch [53400/200000], Loss: 0.0386\n",
            "Epoch [53500/200000], Loss: 0.0966\n",
            "Epoch [53600/200000], Loss: 0.0416\n",
            "Epoch [53700/200000], Loss: 0.0345\n",
            "Epoch [53800/200000], Loss: 0.0466\n",
            "Epoch [53900/200000], Loss: 0.0638\n",
            "Epoch [54000/200000], Loss: 0.1610\n",
            "Epoch [54100/200000], Loss: 0.5655\n",
            "Epoch [54200/200000], Loss: 0.0342\n",
            "Epoch [54300/200000], Loss: 0.0319\n",
            "Epoch [54400/200000], Loss: 0.0386\n",
            "Epoch [54500/200000], Loss: 0.0415\n",
            "Epoch [54600/200000], Loss: 0.1258\n",
            "Epoch [54700/200000], Loss: 0.1311\n",
            "Epoch [54800/200000], Loss: 0.3817\n",
            "Epoch [54900/200000], Loss: 0.0293\n",
            "Epoch [55000/200000], Loss: 0.0330\n",
            "Epoch [55100/200000], Loss: 0.4549\n",
            "Epoch [55200/200000], Loss: 0.0296\n",
            "Epoch [55300/200000], Loss: 0.0291\n",
            "Epoch [55400/200000], Loss: 0.0375\n",
            "Epoch [55500/200000], Loss: 0.3211\n",
            "Epoch [55600/200000], Loss: 0.1396\n",
            "Epoch [55700/200000], Loss: 0.0380\n",
            "Epoch [55800/200000], Loss: 0.1100\n",
            "Epoch [55900/200000], Loss: 0.0498\n",
            "Epoch [56000/200000], Loss: 0.1743\n",
            "Epoch [56100/200000], Loss: 0.0280\n",
            "Epoch [56200/200000], Loss: 0.0326\n",
            "Epoch [56300/200000], Loss: 0.0541\n",
            "Epoch [56400/200000], Loss: 0.0411\n",
            "Epoch [56500/200000], Loss: 0.0931\n",
            "Epoch [56600/200000], Loss: 0.0306\n",
            "Epoch [56700/200000], Loss: 0.0486\n",
            "Epoch [56800/200000], Loss: 0.0358\n",
            "Epoch [56900/200000], Loss: 0.1270\n",
            "Epoch [57000/200000], Loss: 0.0284\n",
            "Epoch [57100/200000], Loss: 0.3055\n",
            "Epoch [57200/200000], Loss: 0.1236\n",
            "Epoch [57300/200000], Loss: 0.0277\n",
            "Epoch [57400/200000], Loss: 0.1268\n",
            "Epoch [57500/200000], Loss: 0.1921\n",
            "Epoch [57600/200000], Loss: 0.0904\n",
            "Epoch [57700/200000], Loss: 0.0842\n",
            "Epoch [57800/200000], Loss: 0.0839\n",
            "Epoch [57900/200000], Loss: 0.0434\n",
            "Epoch [58000/200000], Loss: 0.0840\n",
            "Epoch [58100/200000], Loss: 0.0591\n",
            "Epoch [58200/200000], Loss: 0.0611\n",
            "Epoch [58300/200000], Loss: 0.0271\n",
            "Epoch [58400/200000], Loss: 0.5854\n",
            "Epoch [58500/200000], Loss: 0.6365\n",
            "Epoch [58600/200000], Loss: 0.0323\n",
            "Epoch [58700/200000], Loss: 0.0474\n",
            "Epoch [58800/200000], Loss: 0.3358\n",
            "Epoch [58900/200000], Loss: 0.0279\n",
            "Epoch [59000/200000], Loss: 0.0612\n",
            "Epoch [59100/200000], Loss: 0.0375\n",
            "Epoch [59200/200000], Loss: 0.0411\n",
            "Epoch [59300/200000], Loss: 0.0334\n",
            "Epoch [59400/200000], Loss: 0.4890\n",
            "Epoch [59500/200000], Loss: 0.0809\n",
            "Epoch [59600/200000], Loss: 0.0487\n",
            "Epoch [59700/200000], Loss: 0.1069\n",
            "Epoch [59800/200000], Loss: 0.0781\n",
            "Epoch [59900/200000], Loss: 0.1273\n",
            "Epoch [60000/200000], Loss: 0.0324\n",
            "Epoch [60100/200000], Loss: 0.0288\n",
            "Epoch [60200/200000], Loss: 0.2621\n",
            "Epoch [60300/200000], Loss: 0.0284\n",
            "Epoch [60400/200000], Loss: 0.0307\n",
            "Epoch [60500/200000], Loss: 0.1327\n",
            "Epoch [60600/200000], Loss: 0.0285\n",
            "Epoch [60700/200000], Loss: 0.1792\n",
            "Epoch [60800/200000], Loss: 0.0377\n",
            "Epoch [60900/200000], Loss: 0.0382\n",
            "Epoch [61000/200000], Loss: 0.0760\n",
            "Epoch [61100/200000], Loss: 0.1110\n",
            "Epoch [61200/200000], Loss: 0.0820\n",
            "Epoch [61300/200000], Loss: 0.0434\n",
            "Epoch [61400/200000], Loss: 0.0335\n",
            "Epoch [61500/200000], Loss: 0.4953\n",
            "Epoch [61600/200000], Loss: 0.0392\n",
            "Epoch [61700/200000], Loss: 0.0415\n",
            "Epoch [61800/200000], Loss: 0.0329\n",
            "Epoch [61900/200000], Loss: 0.0890\n",
            "Epoch [62000/200000], Loss: 0.0748\n",
            "Epoch [62100/200000], Loss: 0.0685\n",
            "Epoch [62200/200000], Loss: 0.0328\n",
            "Epoch [62300/200000], Loss: 0.0672\n",
            "Epoch [62400/200000], Loss: 0.0503\n",
            "Epoch [62500/200000], Loss: 0.0878\n",
            "Epoch [62600/200000], Loss: 0.0551\n",
            "Epoch [62700/200000], Loss: 0.0686\n",
            "Epoch [62800/200000], Loss: 0.0260\n",
            "Epoch [62900/200000], Loss: 0.0262\n",
            "Epoch [63000/200000], Loss: 0.1956\n",
            "Epoch [63100/200000], Loss: 0.5319\n",
            "Epoch [63200/200000], Loss: 0.1198\n",
            "Epoch [63300/200000], Loss: 0.0609\n",
            "Epoch [63400/200000], Loss: 0.0792\n",
            "Epoch [63500/200000], Loss: 0.0937\n",
            "Epoch [63600/200000], Loss: 0.0802\n",
            "Epoch [63700/200000], Loss: 0.2128\n",
            "Epoch [63800/200000], Loss: 0.0571\n",
            "Epoch [63900/200000], Loss: 0.3533\n",
            "Epoch [64000/200000], Loss: 0.0272\n",
            "Epoch [64100/200000], Loss: 0.5330\n",
            "Epoch [64200/200000], Loss: 0.0452\n",
            "Epoch [64300/200000], Loss: 0.0563\n",
            "Epoch [64400/200000], Loss: 0.0428\n",
            "Epoch [64500/200000], Loss: 0.3678\n",
            "Epoch [64600/200000], Loss: 0.0459\n",
            "Epoch [64700/200000], Loss: 0.0575\n",
            "Epoch [64800/200000], Loss: 0.0397\n",
            "Epoch [64826], Early stopping with loss: 0.0201\n",
            "Enter the first number (1-12, or 0 to stop): 12\n",
            "Enter the second number (1-12, or 0 to stop): 12\n",
            "The result of 12 multiplied by 12 is approximately: 143.54995727539062\n",
            " \n",
            "The result of 12 multiplied by 12 is approximately: 144\n",
            "***************************************\n",
            "Enter the first number (1-12, or 0 to stop): 8\n",
            "Enter the second number (1-12, or 0 to stop): 7\n",
            "The result of 8 multiplied by 7 is approximately: 55.590362548828125\n",
            " \n",
            "The result of 8 multiplied by 7 is approximately: 56\n",
            "***************************************\n",
            "Enter the first number (1-12, or 0 to stop): 7\n",
            "Enter the second number (1-12, or 0 to stop): 3\n",
            "The result of 7 multiplied by 3 is approximately: 20.916528701782227\n",
            " \n",
            "The result of 7 multiplied by 3 is approximately: 21\n",
            "***************************************\n",
            "Enter the first number (1-12, or 0 to stop): 7\n",
            "Enter the second number (1-12, or 0 to stop): 9\n",
            "The result of 7 multiplied by 9 is approximately: 62.680179595947266\n",
            " \n",
            "The result of 7 multiplied by 9 is approximately: 63\n",
            "***************************************\n",
            "Enter the first number (1-12, or 0 to stop): 9\n",
            "Enter the second number (1-12, or 0 to stop): 9\n",
            "The result of 9 multiplied by 9 is approximately: 81.24636840820312\n",
            " \n",
            "The result of 9 multiplied by 9 is approximately: 81\n",
            "***************************************\n",
            "Enter the first number (1-12, or 0 to stop): 5\n",
            "Enter the second number (1-12, or 0 to stop): 8\n",
            "The result of 5 multiplied by 8 is approximately: 39.86505126953125\n",
            " \n",
            "The result of 5 multiplied by 8 is approximately: 40\n",
            "***************************************\n",
            "Enter the first number (1-12, or 0 to stop): 0\n",
            "Enter the second number (1-12, or 0 to stop): 0\n",
            "Exiting testing loop.\n",
            "Enter the number of epochs for training: 0\n",
            "Enter the loss threshold for early stopping: 0\n",
            "Enter the first number (1-12, or 0 to stop): 0\n",
            "Enter the second number (1-12, or 0 to stop): 0\n",
            "Exiting testing loop.\n"
          ]
        }
      ]
    }
  ]
}